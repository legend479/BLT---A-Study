# BLT: Byte Latent Transformer - Complete Implementation

A comprehensive implementation of BLT (Byte Latent Transformer) with automated hyperparameter tuning, training, evaluation, and analysis pipeline.

## ğŸš€ Quick Start

### One-Command Complete Experiment
```bash
# Full experiment with hyperparameter tuning (2-4 hours)
python run_experiments.py --tune

# Quick test (10-20 minutes)
python run_experiments.py --quick

# Full training with default parameters (30-60 minutes)
python run_experiments.py --full
```

### Manual Training
```bash
# Train BLT model
python src/train.py --mode blt --epochs 10 --batch_size 32 --use_amp

# Train Baseline model
python src/train.py --mode char --epochs 10 --batch_size 32 --use_amp

# Evaluate models
python src/eval.py --mode blt --checkpoint checkpoints/best_blt_model.pt --test_csv data/test.csv
python src/eval.py --mode char --checkpoint checkpoints/best_char_model.pt --test_csv data/test.csv

# Generate analysis
python analyze_results.py
```

## ğŸ“ Project Structure

```
BLT---A-Study/
â”œâ”€â”€ src/                          # Core implementation
â”‚   â”œâ”€â”€ train.py                  # Training script with optimizations
â”‚   â”œâ”€â”€ eval.py                   # Evaluation script
â”‚   â”œâ”€â”€ blt_model.py             # BLT model (optimized)
â”‚   â”œâ”€â”€ baseline_model.py        # Baseline transformer
â”‚   â”œâ”€â”€ dataset.py               # Data processing (optimized)
â”‚   â”œâ”€â”€ patcher.py               # Entropy-based patching
â”‚   â”œâ”€â”€ tokenizer.py             # Character tokenizer
â”‚   â””â”€â”€ infer.py                 # Interactive inference
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                # Training data (10,001 samples)
â”‚   â”œâ”€â”€ test.csv                 # Test data (2,001 samples)
â”‚   â””â”€â”€ processed/               # Cached preprocessed data
â”œâ”€â”€ run_experiments.py           # Complete automated pipeline
â”œâ”€â”€ analyze_results.py           # Analysis and visualization
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ QUICKSTART.md               # Detailed usage guide
â”œâ”€â”€ EXPERIMENTAL_GUIDE.md       # Research methodology guide
â””â”€â”€ requirements.txt            # Dependencies
```

## ğŸ”¬ Key Features

### BLT Model Optimizations
- **Vectorized Patch Processing**: 3-5x faster than sequential processing
- **Entropy-based Byte Patching**: Dynamic segmentation with Shannon entropy
- **N-gram Hashing**: Efficient representation using 1, 2, 3-gram hashes
- **Memory-Efficient**: Optimized tensor operations and GPU memory management

### Training Optimizations
- **Mixed Precision Training**: Automatic mixed precision with AMP
- **Gradient Accumulation**: Support for larger effective batch sizes
- **Advanced LR Scheduling**: Cosine annealing and step scheduling
- **Multi-GPU Support**: DataParallel with proper state management
- **Auto-System Optimization**: Automatic settings based on hardware

### Automated Pipeline
- **Hyperparameter Tuning**: Grid search with intelligent sampling
- **Resume Capability**: Continue from interruptions automatically
- **Comprehensive Reporting**: Research-grade analysis and statistics
- **End-to-End Automation**: Single command for complete experiments

## ğŸ¯ Usage Modes

### 1. Research Mode (Recommended)
```bash
python run_experiments.py --tune
```
**What it does:**
- Hyperparameter tuning for both models (12 combinations each)
- Training with best parameters (10 epochs)
- Comprehensive evaluation with beam search
- Statistical analysis and visualization
- Research-grade report generation

**Time:** 2-4 hours | **Output:** Complete research analysis

### 2. Quick Validation
```bash
python run_experiments.py --quick
```
**What it does:**
- Fast training (2 epochs) with default parameters
- Basic evaluation and comparison
- Quick validation of implementation

**Time:** 10-20 minutes | **Output:** Basic results

### 3. Full Training
```bash
python run_experiments.py --full
```
**What it does:**
- Complete training (10 epochs) with default parameters
- Comprehensive evaluation
- Detailed analysis and reporting

**Time:** 30-60 minutes | **Output:** Full comparison

### 4. Custom Configuration
```bash
python run_experiments.py --tune --tune_epochs 5 --max_tune_combinations 20
```
**What it does:**
- Custom hyperparameter tuning (5 epochs, 20 combinations)
- Extended search for optimal parameters
- More thorough but longer experiment

**Time:** 4-8 hours | **Output:** Extensive analysis

## ğŸ“Š Generated Outputs

### Research Files
```
results/
â”œâ”€â”€ comprehensive_results.json          # All experimental data
â”œâ”€â”€ experimental_report.txt             # Detailed research report
â”œâ”€â”€ hyperparameter_tuning_blt.json     # BLT tuning results
â”œâ”€â”€ hyperparameter_tuning_char.json    # Baseline tuning results
â””â”€â”€ hyperparameter_analysis.png        # Parameter impact visualization
```

### Model Outputs
```
checkpoints/
â”œâ”€â”€ best_blt_model.pt                   # Best BLT model
â”œâ”€â”€ best_char_model.pt                  # Best baseline model
â”œâ”€â”€ history_blt.json                    # BLT training history
â””â”€â”€ history_char.json                   # Baseline training history
```

### Evaluation Results
```
predictions/
â”œâ”€â”€ predictions_blt.csv                 # BLT predictions
â”œâ”€â”€ predictions_baseline.csv            # Baseline predictions
â”œâ”€â”€ predictions_blt_metrics.json        # BLT detailed metrics
â”œâ”€â”€ predictions_baseline_metrics.json   # Baseline detailed metrics
â”œâ”€â”€ training_curves.png                 # Training dynamics
â”œâ”€â”€ accuracy_analysis.png               # Performance breakdown
â”œâ”€â”€ detailed_metrics.png                # Comprehensive metrics
â””â”€â”€ summary_report.txt                  # Analysis summary
```

## ğŸ”§ Advanced Usage

### Resume Interrupted Experiments
The pipeline automatically resumes from where it left off:
```bash
# If interrupted during tuning, simply run again
python run_experiments.py --tune
# Will resume from last completed tuning run
```

### Custom Hyperparameter Grid
Edit `get_hyperparameter_grid()` in `run_experiments.py`:
```python
def get_hyperparameter_grid():
    return {
        'lr': [1e-4, 5e-4, 1e-3, 2e-3, 5e-3],  # More learning rates
        'd_model': [64, 128, 256, 512],         # Larger models
        'batch_size': [8, 16, 32, 64, 128],     # More batch sizes
        # ... customize as needed
    }
```

### Individual Model Training
```bash
# BLT with custom parameters
python src/train.py --mode blt --epochs 20 --batch_size 64 --lr 2e-3 --d_model 256 --use_amp --lr_scheduler cosine

# Baseline with early stopping
python src/train.py --mode char --epochs 15 --early_stopping_patience 3 --use_amp
```

### Interactive Inference
```bash
# Test BLT model interactively
python src/infer.py --mode blt --checkpoint checkpoints/best_blt_model.pt

# Test with beam search
python src/infer.py --mode char --checkpoint checkpoints/best_char_model.pt --beam_width 5
```

## ğŸ“ˆ Performance Improvements

| Component | Optimization | Performance Gain |
|-----------|-------------|------------------|
| BLT Patch Processing | Vectorization | 3-5x faster |
| Training Speed | Mixed Precision | 1.5-2x faster |
| Memory Usage | Optimization | 30-50% reduction |
| Data Loading | Efficient DataLoader | 20-30% faster |
| Multi-GPU | Proper DataParallel | 2-4x scaling |

## ğŸ“ For Academic Use

### Research Report
The generated `results/experimental_report.txt` includes:
- **Experimental Configuration**: All parameters and settings
- **Hyperparameter Analysis**: Best parameters and insights
- **Statistical Analysis**: Significance testing and confidence intervals
- **Performance Breakdown**: Detailed metrics by input length
- **Key Findings**: Automated analysis of results
- **Conclusions**: Research implications and recommendations

### Citation-Ready Results
- Comprehensive methodology documentation
- Statistical significance testing
- Reproducible experimental setup
- Publication-ready visualizations

## ğŸ›  System Requirements

### Minimum Requirements
- Python 3.8+
- 8GB RAM
- 4GB disk space

### Recommended for Research
- Python 3.9+
- 16GB+ RAM
- GPU with 8GB+ VRAM
- 10GB disk space

### Auto-Optimization
The system automatically:
- Detects GPU memory and adjusts batch sizes
- Enables mixed precision on compatible hardware
- Optimizes data loading based on CPU cores
- Prevents OOM with intelligent memory management

## ğŸ” Troubleshooting

### Common Issues

**Out of Memory:**
```bash
# Reduce batch size and use gradient accumulation
python run_experiments.py --tune --max_tune_combinations 6
```

**Slow Training:**
```bash
# Enable mixed precision (auto-enabled on modern GPUs)
python src/train.py --mode blt --use_amp --batch_size 64
```

**Resume Failed Experiment:**
```bash
# Simply run the same command again - it will resume automatically
python run_experiments.py --tune
```

**Missing Dependencies:**
```bash
pip install -r requirements.txt
```

### Performance Tips

1. **Use GPU**: Training is 10-20x faster on GPU
2. **Enable AMP**: Mixed precision reduces memory and increases speed
3. **Optimize Batch Size**: Larger batches (if memory allows) train faster
4. **Use Multiple Workers**: Increase `--num_workers` for faster data loading

## ğŸ“š Implementation Details

### BLT Model Architecture
- **Entropy-based Patching**: Window size 10, threshold 2.0 bits
- **N-gram Hashing**: 4096 buckets per n-gram size (1, 2, 3)
- **Patch Embeddings**: Separate embedding tables per n-gram
- **Transformer**: Standard encoder-decoder with multi-head attention

### Baseline Model
- **Character-level**: Standard token-by-token processing
- **Transformer**: Same architecture as BLT decoder for fair comparison
- **Positional Embeddings**: Learned position encodings

### Training Features
- **Gradient Clipping**: Prevents exploding gradients
- **Weight Decay**: L2 regularization (0.01)
- **Early Stopping**: Configurable patience
- **Checkpointing**: Best and latest model saving
- **Learning Rate Scheduling**: Cosine annealing, step, or constant

## ğŸ”¬ Research Methodology

The implementation follows rigorous research practices:

1. **Controlled Comparison**: Same architecture for fair evaluation
2. **Hyperparameter Optimization**: Grid search for both models
3. **Statistical Analysis**: Significance testing and confidence intervals
4. **Reproducibility**: Fixed seeds and deterministic operations
5. **Comprehensive Metrics**: Multiple evaluation criteria
6. **Error Analysis**: Detailed breakdown by input characteristics

## ğŸ“„ License and Citation

This implementation is based on the BLT paper:
```
Byte Latent Transformer: Patches Scale Better Than Tokens
https://arxiv.org/pdf/2412.09871
```

For academic use, please cite both the original paper and acknowledge this implementation in your work.

---

**Ready to start?** Run `python run_experiments.py --tune` for the complete research experience!